{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07d1e00",
   "metadata": {},
   "source": [
    "\n",
    "## **1. Introduzione Tecnica: Il Motore sotto il Cofano**\n",
    "\n",
    "Molti praticanti usano XGBoost come una \"black box\", ma l'efficacia del tuning dipende dalla comprensione di due pilastri: l'approccio matematico di **secondo ordine** e l'**ottimizzazione di sistema**.\n",
    "\n",
    "### **1.1 Oltre il classico Gradient Boosting (GBM)**\n",
    "\n",
    "Il Gradient Boosting standard è un metodo \"additivo\". Invece di ottimizzare i parametri di un singolo modello complesso (come in una rete neurale), costruiamo un insieme di modelli \"deboli\" (alberi decisionali) in sequenza.\n",
    "\n",
    "Matematicamente, la predizione al passo $t$ per l'istanza $i$ è data da:\n",
    "\n",
    "$$\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + \\eta f_t(x_i)$$\n",
    "\n",
    "Dove:\n",
    "* $\\hat{y}_i^{(t-1)}$ è la predizione accumulata fino al passo precedente.\n",
    "* $f_t(x_i)$ è il nuovo albero che stiamo aggiungendo.\n",
    "* $\\eta$ è il learning rate.\n",
    "\n",
    "\n",
    "\n",
    "**La differenza cruciale:**\n",
    "In un GBM standard, il nuovo albero $f_t$ viene addestrato per predire i **residui** (o il gradiente negativo della loss) del modello precedente. Usa il metodo della *Discesa del Gradiente* (Primo Ordine).\n",
    "\n",
    "**XGBoost**, invece, utilizza il **Metodo di Newton** (Secondo Ordine). Quando ottimizza la funzione obiettivo per trovare il miglior albero successivo, non guarda solo la pendenza (gradiente), ma anche la **curvatura** della funzione di loss.\n",
    "\n",
    "### **1.2 L'Espansione di Taylor: Il cuore di XGBoost**\n",
    "\n",
    "Per capire i parametri che vedremo dopo (come `min_child_weight`), dobbiamo guardare l'approssimazione della Loss Function che XGBoost utilizza.\n",
    "\n",
    "XGBoost approssima la funzione obiettivo usando l'**Espansione di Taylor di secondo ordine**:\n",
    "\n",
    "$$\\mathcal{L}^{(t)} \\approx \\sum_{i=1}^n \\left[ l(y_i, \\hat{y}^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2}h_i f_t^2(x_i) \\right] + \\Omega(f_t)$$\n",
    "\n",
    "Qui risiedono i due concetti più importanti per un utente avanzato:\n",
    "\n",
    "1.  **$g_i$ (Gradiente):** La derivata prima (la direzione verso cui muoversi).\n",
    "2.  **$h_i$ (Hessiana):** La derivata seconda (la curvatura). Rappresenta quanto siamo \"sicuri\" o quanto rapidamente sta cambiando il gradiente.\n",
    "\n",
    "> **Insight da Esperto:** Quando tunerai `min_child_weight`, starai letteralmente impostando una soglia sulla somma delle Hessiane ($h_i$) in una foglia. In una regressione MSE, l'Hessiana è costante ($2$), quindi `min_child_weight` è semplicemente il numero di campioni. Ma in una classificazione logistica, l'Hessiana diventa piccola quando il modello è molto sicuro (probabilità vicine a 0 o 1). Ecco perché XGBoost è così preciso: **pesa gli errori in base alla \"sicurezza\" del modello corrente.**\n",
    "\n",
    "### **1.3 Regolarizzazione Integrata ($\\Omega$)**\n",
    "\n",
    "A differenza di altre librerie che applicano la regolarizzazione come ripensamento (post-pruning), XGBoost include il termine di regolarizzazione $\\Omega(f_t)$ direttamente nella funzione obiettivo durante la costruzione dell'albero:\n",
    "\n",
    "$$\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda ||w||^2$$\n",
    "\n",
    "* **$\\gamma T$**: Penalizza il numero di foglie ($T$). Più foglie = costo maggiore. Questo è controllato dal parametro `gamma`.\n",
    "* **$\\lambda ||w||^2$**: Penalizza la grandezza dei pesi delle foglie ($w$, i valori predetti nelle foglie). Questo è controllato dal parametro `lambda`.\n",
    "\n",
    "L'albero non cresce se il guadagno di loss non supera la penalità della complessità.\n",
    "\n",
    "### **1.4 Ottimizzazione di Sistema (Engineering)**\n",
    "\n",
    "XGBoost non è famoso solo per la matematica, ma per come gestisce l'hardware:\n",
    "\n",
    "* **Block Structure & Parallelization:** Un errore comune è pensare che XGBoost costruisca alberi in parallelo. Non può (è sequenziale). Invece, parallelizza la **costruzione dei nodi**. I dati sono pre-ordinati e salvati in blocchi di memoria (CSC format), permettendo a più thread di calcolare i migliori split simultaneamente.\n",
    "* **Weighted Quantile Sketch:** Per dataset enormi, è impossibile testare ogni possibile valore di split. XGBoost usa un algoritmo di sketch approssimato per trovare i migliori punti di taglio candidati, pesati dall'Hessiana ($h_i$).\n",
    "* **Sparsity Awareness:** XGBoost impara una \"direzione di default\" per ogni nodo. Se un dato è mancante (NaN), viene instradato automaticamente nella direzione che minimizza l'errore di training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb047b",
   "metadata": {},
   "source": [
    "## **2. Anatomia Pratica degli Iperparametri (Il \"Booster\")**\n",
    "\n",
    "In un contesto avanzato, non si settano i parametri a caso. Bisogna vedere gli iperparametri come manopole che controllano tre aspetti: **Capacità del modello**, **Robustezza al rumore** e **Velocità di convergenza**.\n",
    "\n",
    "Ecco come un esperto configura il \"Booster\" (`gbtree`), andando oltre i valori di default.\n",
    "\n",
    "### **2.1 Controllo della Struttura (Il freno e l'acceleratore)**\n",
    "\n",
    "Questi parametri definiscono la \"forma\" fisica dei tuoi alberi.\n",
    "\n",
    "* **`max_depth` (Profondità dell'albero)**\n",
    "    * **Cosa fa davvero:** Controlla la complessità delle interazioni tra le feature che il modello può apprendere. Un albero di profondità $N$ può catturare interazioni fino a $N$ variabili.\n",
    "    * **Utilizzo Avanzato:**\n",
    "        * **Non esagerare:** In XGBoost, a differenza di Random Forest, gli alberi non devono essere profondi. Spesso un range **3-6** è ottimale.\n",
    "        * **Segnale vs Rumore:** Se aumenti la profondità sopra a 8-10, stai quasi certamente memorizzando il rumore, a meno che tu non abbia milioni di righe e interazioni feature estremamente complesse.\n",
    "        * **Interazione:** È fortemente correlato a `min_child_weight`. Se alzi la profondità, *devi* alzare `min_child_weight` per evitare foglie con pochi dati.\n",
    "\n",
    "* **`min_child_weight` (Il \"Filtro anti-rumore\")**\n",
    "    * **Cosa fa davvero:** È la somma minima dei pesi (hessiana) necessaria per mantenere un nodo figlio. In pratica (per regressione/classificazione standard), puoi pensarlo come il numero minimo di istanze necessarie in una foglia per validare uno split.\n",
    "    * **Utilizzo Avanzato:**\n",
    "        * **Dataset sbilanciati:** Questo è il parametro più critico. Se hai classi rare, un valore alto impedirà al modello di isolarle.\n",
    "        * **Dataset rumorosi:** Se il tuo dataset ha molto rumore (es. dati finanziari o sensori IoT), alza questo valore (es. 10, 20 o anche 100). Costringe l'albero a fare split solo su pattern molto \"solidi\" e frequenti.\n",
    "        * **Rule of Thumb:** Inizia con 1. Se vedi overfitting massiccio (Train score >> Test score), prova subito a saltare a 5 o 10.\n",
    "\n",
    "* **`gamma` (o `min_split_loss`) - Il Pruning Aggressivo**\n",
    "    * **Cosa fa davvero:** È una soglia \"hard\". Se lo split non riduce la loss function di almeno `gamma`, lo split non avviene. È una regolarizzazione che agisce *durante* la costruzione, non dopo.\n",
    "    * **Utilizzo Avanzato:**\n",
    "        * **Default vs Realtà:** Il default è 0 (crescita greedy).\n",
    "        * **Quando usarlo:** È utilissimo quando il modello continua a creare rami inutili che migliorano di pochissimo la performance. Impostare un gamma basso (es. 0.1 - 0.5) rende il modello molto più conservativo (ottimo per evitare overfitting in produzione).\n",
    "        * **Tuning:** È difficile da tunare con GridSearch perché dipende dalla scala della tua loss. Meglio lasciarlo a 0 all'inizio e alzarlo solo se la regolarizzazione standard (`lambda`/`alpha`) non basta.\n",
    "\n",
    "### **2.2 Campionamento Stocastico (La diversità)**\n",
    "\n",
    "Questi parametri introducono casualità. Senza di questi, XGBoost è deterministico. La casualità riduce la correlazione tra gli alberi, migliorando l'ensemble (meno varianza).\n",
    "\n",
    "* **`subsample` (Righe)**\n",
    "    * **Pratica:** Percentuale di righe campionate per costruire ogni albero.\n",
    "    * **Sweet Spot:** Generalmente tra **0.6 e 0.9**.\n",
    "    * **Warning:** Non scendere mai sotto 0.5 a meno che il dataset non sia enorme. Impostarlo a 1.0 (default) spesso porta a overfitting perché ogni albero vede esattamente gli stessi dati.\n",
    "\n",
    "* **`colsample_bytree` (Colonne)**\n",
    "    * **Pratica:** Percentuale di colonne (feature) scelte a caso per costruire ogni albero. Simile al `max_features` di Random Forest.\n",
    "    * **Utilizzo Avanzato:**\n",
    "        * Questo è spesso **più efficace della regolarizzazione L1/L2**.\n",
    "        * Se hai molte feature collineari (ridondanti), abbassa questo valore (es. 0.6). Costringe gli alberi a usare feature diverse, rendendo il modello finale più robusto se una feature dovesse \"rompersi\" in produzione.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **2.3 La Strategia di Tuning Professionale (\"The Recipe\")**\n",
    "\n",
    "Non lanciare una GridSearch cieca su tutti i parametri. Sprechi CPU. Usa questo approccio a imbuto:\n",
    "\n",
    "1.  **Fase 1: Baseline Veloce**\n",
    "    * Fissa un `learning_rate` alto (es. 0.1 o 0.2) per addestrare velocemente.\n",
    "    * Trova il numero ottimale di alberi (`n_estimators`) usando `early_stopping`.\n",
    "\n",
    "2.  **Fase 2: Struttura dell'Albero (Macro-tuning)**\n",
    "    * Tuna `max_depth` e `min_child_weight` insieme. Sono i parametri che impattano di più sul risultato.\n",
    "    * *Esempio:* GridSearch su Depth [3, 5, 7, 9] e Child Weight [1, 3, 5].\n",
    "\n",
    "3.  **Fase 3: Regolazione fine (Micro-tuning)**\n",
    "    * Tuna `gamma` per potare i rami inutili.\n",
    "    * Tuna `subsample` e `colsample_bytree` per aggiungere robustezza.\n",
    "\n",
    "4.  **Fase 4: Il \"Grand Finale\" (Lower Rate, More Trees)**\n",
    "    * Una volta trovati i parametri strutturali, abbassa il `learning_rate` (es. a 0.01 o 0.005).\n",
    "    * Aumenta proporzionalmente `n_estimators`.\n",
    "    * *Nota da esperto:* Questo passaggio da solo regala spesso un boost di 1-2% di performance, perché permette all'algoritmo di convergere verso il minimo globale con passi più fini, riducendo l'errore residuo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d068a0",
   "metadata": {},
   "source": [
    "## **3. Parametri di Apprendimento e Regolarizzazione**\n",
    "\n",
    "Qui gestiamo due aspetti critici: **come** il modello impara dai propri errori (Learning Rate) e **come** evita di dare troppa importanza a feature rumorose (Regolarizzazione).\n",
    "\n",
    "### **3.1 Il Motore del Boosting: `eta` e `n_estimators`**\n",
    "\n",
    "Questi due parametri vivono in simbiosi. Non puoi modificarne uno senza considerare l'altro.\n",
    "\n",
    "* **`eta` (o `learning_rate`)**\n",
    "    * **Concetto Pratico:** È la \"dimensione del passo\". Dopo ogni albero, XGBoost non aggiunge l'intero valore della predizione, ma lo moltiplica per `eta`. Questo riduce l'impatto di ogni singolo albero, lasciando spazio agli alberi successivi per correggere gli errori.\n",
    "    * **Perché è importante:** Un learning rate basso rende il modello più robusto all'overfitting, poiché la costruzione del modello finale è più graduale e meno dipendente dai primi alberi (che potrebbero aver memorizzato rumore).\n",
    "\n",
    "* **`n_estimators` (o `num_boost_round`)**\n",
    "    * **Concetto Pratico:** Il numero totale di alberi sequenziali da costruire.\n",
    "\n",
    "\n",
    "\n",
    "#### **La Strategia Professionale: \"Shrinkage\"**\n",
    "La regola d'oro nell'industria non è cercare il \"learning rate magico\", ma seguire questa procedura:\n",
    "\n",
    "1.  **Fase di Tuning:** Usa un `eta` alto (es. **0.1** o **0.2**) e un numero di stimatori basso/medio. Questo ti permette di fare decine di test di `max_depth` e `subsample` in pochi minuti invece che ore.\n",
    "2.  **Fase di Produzione:** Una volta trovata la struttura ideale degli alberi, applica la tecnica dello **Shrinkage**:\n",
    "    * Riduci `eta` di un fattore $X$ (es. da 0.1 a **0.01**).\n",
    "    * Aumenta `n_estimators` dello stesso fattore $X$.\n",
    "    * *Risultato:* Il modello impiegherà più tempo ad addestrarsi, ma l'errore di generalizzazione scenderà quasi sempre, migliorando la precisione finale.\n",
    "\n",
    "> **Nota:** Usa sempre **Early Stopping** quando aumenti gli stimatori. Se imposti 10.000 alberi ma il modello smette di migliorare al 3.500esimo, l'early stopping fermerà il training, risparmiando ore di calcolo e prevenendo l'overfitting tardivo.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.2 Regolarizzazione Esplicita (L1 & L2)**\n",
    "\n",
    "Mentre `max_depth` limita la struttura, `lambda` e `alpha` limitano i **pesi** numerici assegnati alle foglie. Questo è fondamentale quando si hanno feature con alta varianza o dataset con molto rumore.\n",
    "\n",
    "* **`lambda` (Regolarizzazione L2 - Ridge)**\n",
    "    * **Default:** 1 (Attivo di default, a differenza di sklearn che spesso non regolarizza).\n",
    "    * **Effetto Pratico:** \"Schiaccia\" i valori delle foglie verso zero in modo fluido. Penalizza i valori estremi.\n",
    "    * **Quando usarlo:** È il tuo \"scudo\" standard. Se vedi che il modello dà pesi enormi a certe predizioni su pochi casi, aumenta `lambda`. Aiuta a gestire la multicollinearità (feature correlate).\n",
    "\n",
    "* **`alpha` (Regolarizzazione L1 - Lasso)**\n",
    "    * **Default:** 0.\n",
    "    * **Effetto Pratico:** Forza i pesi delle feature inutili a diventare **esattamente zero**.\n",
    "    * **Quando usarlo (Feature Selection Implicita):**\n",
    "        * Se hai un dataset con **migliaia di feature** (es. One-Hot Encoding di variabili categoriche ad alta cardinalità o dati genomici) e sospetti che solo poche siano importanti.\n",
    "        * Impostare `alpha` alto rende il modello \"sparso\" (più leggero e veloce in inferenza perché usa meno feature).\n",
    "\n",
    "#### **Scenario d'uso: L1 vs L2**\n",
    "* *Problema classico (es. Churn Prediction):* Usa **L2 (`lambda`)**. Vogliamo considerare tutte le variabili un po'.\n",
    "* *Problema ad alta dimensione (es. Analisi del testo con TF-IDF):* Usa **L1 (`alpha`)**. Vogliamo eliminare le migliaia di parole che non servono a nulla.\n",
    "\n",
    "---\n",
    "\n",
    "**Sintesi del Punto 3:**\n",
    "* **Learning Rate:** Tienilo basso in produzione (0.01 - 0.05).\n",
    "* **Estimators:** Alzali quando abbassi il learning rate (e usa Early Stopping).\n",
    "* **Alpha/Lambda:** Usa Alpha se vuoi selezionare feature, Lambda per stabilità generale.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a742c",
   "metadata": {},
   "source": [
    "\n",
    "## **4. Gestione di Scenari Complessi**\n",
    "\n",
    "### **4.1 Dataset Sbilanciati (Imbalanced Learning)**\n",
    "Quando la classe positiva (quella che ci interessa, es. \"Frode\" o \"Guasto\") è molto rara, il modello tende a ignorarla per massimizzare l'accuratezza globale (dicendo sempre \"Non Frode\").\n",
    "\n",
    "Ecco come forzare XGBoost a prestare attenzione ai casi rari:\n",
    "\n",
    "* **`scale_pos_weight` (Il Bilanciatore)**\n",
    "    * **La Logica:** Modifica il calcolo del gradiente. Se questo parametro è > 1, gli errori commessi sulla classe positiva pesano di più durante l'aggiornamento dei pesi.\n",
    "    * **Formula Magica:**\n",
    "        $$scale\\_pos\\_weight = \\frac{\\text{Numero di Negativi}}{\\text{Numero di Positivi}}$$\n",
    "    * **Utilizzo Professionale:**\n",
    "        * Calcola questo rapporto e inseriscilo nel modello. Spesso è più efficace e veloce delle tecniche di campionamento esterne (come SMOTE o Oversampling) perché non altera la distribuzione dei dati, ma solo la penalità matematica.\n",
    "        * **Attenzione alle Probabilità:** Quando usi `scale_pos_weight`, le probabilità predette (`predict_proba`) non saranno più calibrate (saranno spostate verso l'alto). Se hai bisogno della probabilità reale (es. per calcolare il rischio finanziario esatto), dovrai ricalibrarle a valle.\n",
    "\n",
    "* **`max_delta_step` (La cintura di sicurezza per la convergenza)**\n",
    "    * **Il Problema:** In scenari estremamente sbilanciati, l'aggiornamento dei pesi di un singolo albero può essere enorme (\"esplosivo\"), portando a instabilità numerica.\n",
    "    * **Soluzione:** `max_delta_step` pone un tetto massimo al cambiamento di peso (delta) di una singola foglia.\n",
    "    * **Configurazione:** Il default è 0 (nessun tetto). Se hai problemi di convergenza con classi molto rare, impostalo tra **1 e 10**. Questo rende l'aggiornamento più conservativo e stabile.\n",
    "\n",
    "\n",
    "\n",
    "### **4.2 Gestione dei Valori Mancanti (Sparsity Awareness)**\n",
    "\n",
    "Molti ingegneri perdono tempo a imputare i valori mancanti (Mean, Median, KNN Imputation) prima di passare i dati a XGBoost. Spesso, **questo è un errore**.\n",
    "\n",
    "* **Sparsity Aware Split Finding**\n",
    "    * XGBoost gestisce i `NaN` (Not a Number) nativamente. Non li ignora, li *usa*.\n",
    "    * **Come funziona:** Per ogni nodo (decisione) nell'albero, l'algoritmo testa due scenari:\n",
    "        1.  Manda tutti i dati con valore mancante a **Sinistra**. Calcola il guadagno di info.\n",
    "        2.  Manda tutti i dati con valore mancante a **Destra**. Calcola il guadagno di info.\n",
    "    * La direzione che minimizza la loss viene \"imparata\" e salvata come **Default Direction**.\n",
    "    * **Perché è geniale:** Spesso un dato mancante non è casuale (Missing Not At Random).\n",
    "        * *Esempio:* In un dataset bancario, se il campo \"Debito Pregresso\" è vuoto (`NaN`), potrebbe significare che il cliente non ha mai avuto debiti (ottimo pagatore). Se lo riempi con la media, distruggi questa informazione. XGBoost invece impara che `NaN` -> \"Ramo dei buoni pagatori\".\n",
    "\n",
    "\n",
    "\n",
    "#### **Best Practice sui Dati Mancanti:**\n",
    "1.  **Non imputare nulla** inizialmente. Passa i `NaN` (o `np.nan` in Python) direttamente a XGBoost.\n",
    "2.  Lascia che l'algoritmo scopra se l'assenza di informazione è essa stessa un'informazione (informative missingness).\n",
    "3.  Imputa manualmente solo se sai per certo che il dato manca per un errore tecnico casuale e la media/mediana è una stima affidabile.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9134161c",
   "metadata": {},
   "source": [
    "## **5. Tecniche Avanzate: Ingegnerizzare la Conoscenza**\n",
    "\n",
    "### **5.1 Monotonic Constraints (Vincoli Monotoni)**\n",
    "Spesso sappiamo a priori come una feature dovrebbe influenzare il target.\n",
    "* *Esempio Immobiliare:* A parità di altre condizioni, se i metri quadri aumentano, il prezzo *deve* salire (o rimanere uguale). Non può scendere.\n",
    "* *Il Problema:* Se il training set ha dei dati rumorosi (es. una casa grande svenduta per urgenza), un albero normale potrebbe imparare un \"dip\" (calo) di prezzo per metrature alte. Questo è overfitting su rumore locale.\n",
    "\n",
    "**La Soluzione:**\n",
    "Puoi forzare XGBoost a rispettare una relazione sempre crescente o decrescente per specifiche feature.\n",
    "\n",
    "* **Parametro:** `monotone_constraints`\n",
    "* **Valori:**\n",
    "    * `1`: Relazione crescente (All'aumentare di X, Y aumenta o resta uguale).\n",
    "    * `-1`: Relazione decrescente (All'aumentare di X, Y diminuisce o resta uguale).\n",
    "    * `0`: Nessun vincolo.\n",
    "* **Vantaggi:**\n",
    "    1.  **Migliore Generalizzazione:** Il modello ignora il rumore che contraddice la logica nota.\n",
    "    2.  **Explainability & Trust:** Quando spieghi il modello agli stakeholder, non vedranno comportamenti illogici (es. \"Perché se guadagno di più la banca mi dà meno credito?\").\n",
    "\n",
    "\n",
    "\n",
    "### **5.2 Interaction Constraints (Vincoli di Interazione)**\n",
    "Di default, XGBoost può combinare qualsiasi feature con qualsiasi altra. Ma in certi settori (es. Assicurativo, Credit Risk), alcune interazioni potrebbero essere vietate per legge (discriminazione) o prive di senso logico.\n",
    "\n",
    "* **Parametro:** `interaction_constraints`\n",
    "* **Come funziona:** Passi una lista di liste. Le feature presenti nella stessa sottolista possono interagire tra loro, ma non con feature di altre liste.\n",
    "    * *Esempio:* `[[Feature_A, Feature_B], [Feature_C, Feature_D, Feature_E]]`.\n",
    "    * Qui l'albero può fare split su A e poi su B nello stesso ramo. Ma se ha fatto split su C, non può scendere e fare uno split su A.\n",
    "* **Uso Avanzato:** Riduce drasticamente lo spazio delle ipotesi, prevenendo il modello dal trovare correlazioni spurie complesse che non esistono nella realtà.\n",
    "\n",
    "### **5.3 Custom Objective Functions (Loss Personalizzate)**\n",
    "Questo è il livello \"Gran Maestro\". A volte `RMSE` (Regressione) o `LogLoss` (Classificazione) non riflettono il vero obiettivo di business.\n",
    "\n",
    "* **Scenario (Asymmetric Loss):** Immagina di predire la domanda di magazzino.\n",
    "    * Sottostimare la domanda (Stock-out) costa 1000€ in vendite perse.\n",
    "    * Sovrastimare la domanda (Over-stock) costa 50€ di stoccaggio.\n",
    "    * L'errore quadratico medio (MSE) tratterebbe +10 e -10 allo stesso modo. Per il business, -10 è un disastro.\n",
    "\n",
    "* **Implementazione:**\n",
    "    XGBoost permette di definire una funzione Python personalizzata che calcola e restituisce due vettori per ogni istanza di training:\n",
    "    1.  **Gradiente (Gradient):** La derivata prima (Direzione dell'errore).\n",
    "    2.  **Hessiana (Hessian):** La derivata seconda (Curvatura/Peso dell'errore).\n",
    "\n",
    "Definendo una funzione che penalizza fortemente il gradiente quando l'errore è negativo (stock-out) e poco quando è positivo, guidi l'apprendimento verso una strategia di \"prudente sovrastima\".\n",
    "\n",
    "### **5.4 Supporto Nativo per Feature Categoriche**\n",
    "Fino a poco tempo fa, dovevi fare *One-Hot Encoding* (OHE) prima di usare XGBoost. Questo creava matrici sparse enormi e alberi sbilanciati (perché per isolare la categoria \"Z\" servivano tanti split se usavi OHE).\n",
    "\n",
    "* **La Novità:** XGBoost ora supporta `enable_categorical=True` (con tree method `hist` o `gpu_hist`).\n",
    "* **Come funziona (Optimal Partitioning):** Invece di trattare le categorie come numeri, l'algoritmo cerca la partizione ottimale delle categorie in due gruppi a ogni nodo.\n",
    "    * *Split:* \"È la categoria {A, C, F}?\" vs \"È la categoria {B, D, E}?\".\n",
    "* **Vantaggio:**\n",
    "    * Addestramento molto più veloce su dataset con alta cardinalità.\n",
    "    * Spesso performance superiori rispetto a OHE o Label Encoding, specialmente con alberi poco profondi.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db42bb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## **6. Strategie di Tuning e Best Practices (Il Metodo Optuna)**\n",
    "\n",
    "### **6.1 Perché Optuna cambia le regole del gioco**\n",
    "\n",
    "`GridSearch` prova tutte le combinazioni ciecamente. `Optuna` impara dal passato.\n",
    "Usa un algoritmo chiamato **TPE (Tree-structured Parzen Estimator)**.\n",
    "\n",
    "In parole semplici:\n",
    "\n",
    "1.  Optuna lancia un training con parametri a caso.\n",
    "2.  Osserva il risultato.\n",
    "3.  Costruisce un modello probabilistico interno che dice: \"Quando `max_depth` è basso e `eta` è alto, il modello fa schifo. Non proverò più lì. Invece, sembra che `subsample` alto funzioni bene, esplorerò di più quella zona.\"\n",
    "4.  **Pruning (Potatura):** Se un trial (tentativo) sta andando male dopo 10 iterazioni, Optuna lo uccide subito. Non spreca risorse per arrivare alla fine di un training fallimentare.\n",
    "\n",
    "### **6.2 Definire lo Spazio di Ricerca (Search Space)**\n",
    "\n",
    "Prima del codice, ecco come un esperto definisce i range. Nota l'uso della scala logaritmica.\n",
    "\n",
    "  * `learning_rate`: **Logaritmicamente** tra 1e-3 e 0.3. È fondamentale perché l'impatto varia per ordini di grandezza.\n",
    "  * `max_depth`: Intero tra 3 e 10 (o 12).\n",
    "  * `min_child_weight`: Intero tra 1 e 10 (per il rumore).\n",
    "  * `subsample` / `colsample_bytree`: Float tra 0.5 e 1.0.\n",
    "  * `reg_lambda` / `reg_alpha`: **Logaritmicamente** tra 1e-8 e 10.0.\n",
    "\n",
    "### **6.3 Il Codice: Template Professionale per XGBoost + Optuna**\n",
    "\n",
    "Questo script non è solo un esempio, è un template pronto per la produzione. Include il **Pruning**, che è la parte che accelera il tuning del 50-70%.\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Definizione della Funzione Obiettivo\n",
    "def objective(trial, X, y):\n",
    "    \n",
    "    # A. Split veloce per validazione interna al trial\n",
    "    # Nota: In produzione, potresti usare StratifiedKFold qui dentro per maggiore robustezza\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25)\n",
    "    \n",
    "    # B. Definizione dello Spazio degli Iperparametri (Dynamic Search Space)\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic', # o 'reg:squarederror' per regressione\n",
    "        'tree_method': 'hist',          # Usa 'gpu_hist' se hai una GPU! Velocizza di 10x.\n",
    "        \n",
    "        # Struttura dell'albero\n",
    "        # Suggeriamo un intero per la profondità\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        # Suggeriamo un intero per il peso minimo (controllo rumore/outlier)\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        # Gamma: soglia minima di riduzione loss per split\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        \n",
    "        # Campionamento (Stochastic Gradient Boosting)\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        \n",
    "        # Regolarizzazione (Scala Logaritmica è cruciale qui)\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "        \n",
    "        # Learning Rate e Estimators\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': 1000 # Impostiamo un tetto alto, l'early stopping lo fermerà prima\n",
    "    }\n",
    "\n",
    "    # C. Inizializzazione del Pruner (Interruzione anticipata dei trial scarsi)\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation_0-logloss\")\n",
    "    \n",
    "    # D. Addestramento del Modello\n",
    "    model = xgb.XGBClassifier(**param)\n",
    "    \n",
    "    model.fit(\n",
    "        train_x, \n",
    "        train_y, \n",
    "        eval_set=[(valid_x, valid_y)], \n",
    "        eval_metric=\"logloss\",\n",
    "        early_stopping_rounds=50, # Ferma se non migliora per 50 round\n",
    "        callbacks=[pruning_callback], # Collega Optuna a XGBoost\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # E. Predizione e calcolo metrica da ottimizzare\n",
    "    preds = model.predict_proba(valid_x)[:, 1] # Probabilità classe 1\n",
    "    loss = log_loss(valid_y, preds)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Esecuzione dello Studio\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Supponiamo di avere X e y caricati\n",
    "# X, y = load_data(...) \n",
    "\n",
    "# 1. Creiamo lo studio (Direction: Minimize LogLoss)\n",
    "study = optuna.create_study(\n",
    "    direction='minimize', \n",
    "    sampler=optuna.samplers.TPESampler(), # Sampler Bayesiano standard\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5) # Uccide i trial peggiori della media\n",
    ")\n",
    "\n",
    "# 2. Avviamo l'ottimizzazione\n",
    "# n_trials: Quante combinazioni provare\n",
    "# timeout: Tempo massimo in secondi (es. 1 ora = 3600)\n",
    "print(\"Inizio Tuning con Optuna...\")\n",
    "study.optimize(lambda trial: objective(trial, X, y), n_trials=100, timeout=3600)\n",
    "\n",
    "# 3. Risultati\n",
    "print(\"Migliori parametri:\", study.best_params)\n",
    "print(\"Miglior Loss:\", study.best_value)\n",
    "\n",
    "# 4. Importanza degli Iperparametri\n",
    "# Optuna ti dice quali parametri hanno influito di più sul risultato!\n",
    "optuna.visualization.plot_param_importances(study).show()\n",
    "```\n",
    "\n",
    "### **6.4 Analisi del Codice: I Dettagli che contano**\n",
    "\n",
    "1.  **`suggest_float(..., log=True)`**: Questa è la chiave. Per parametri come `alpha` o `learning_rate`, la differenza tra 0.001 e 0.01 è enorme (10x), mentre tra 0.8 e 0.81 è nulla. La scala logaritmica permette a Optuna di esplorare gli ordini di grandezza in modo efficiente.\n",
    "2.  **`XGBoostPruningCallback`**: Senza questo, Optuna aspetterebbe la fine dei 1000 alberi per ogni trial. Con questo, se al 50° albero la loss è peggiore della media degli altri trial, Optuna lancia un'eccezione, ferma il training e passa al prossimo set di parametri.\n",
    "3.  **`objective` Function**: Nota come tutto (definizione parametri, training, valutazione) avviene dentro questa funzione. Optuna la chiama ripetutamente.\n",
    "4.  **`MedianPruner`**: Una strategia di pruning semplice ma efficace. Se il trial corrente sta andando peggio della mediana dei trial precedenti allo stesso step, viene tagliato.\n",
    "\n",
    "### **6.5 Workflow Finale: Dal Tuning alla Produzione**\n",
    "\n",
    "Una volta che `study.best_params` ti restituisce il dizionario vincente, non hai finito.\n",
    "\n",
    "1.  **Prendi i migliori parametri.**\n",
    "2.  **Applica la strategia \"Low Rate\" (Shrinkage):**\n",
    "      * Prendi il `learning_rate` suggerito da Optuna e dividilo (es. per 2 o per 5).\n",
    "      * Aumenta `n_estimators` per compensare.\n",
    "3.  **Rialilena sul dataset completo:** Ora usa tutto il dataset (Train + Validation) per addestrare il modello finale che andrà in produzione, usando il numero di step ottimale trovato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f1c3504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Caricamento Dati ---\n",
      "Dataset Shape: (569, 30)\n",
      "Scale Pos Weight calcolato: 0.60\n",
      "\n",
      "--- Training Modello Baseline (Default) ---\n",
      "Baseline AUC Score: 0.9901\n",
      "\n",
      "--- Inizio Tuning con Optuna ---\n",
      "Migliori parametri trovati:\n",
      "{'max_depth': 7, 'min_child_weight': 3, 'gamma': 0.9329375475579135, 'subsample': 0.6906717073754871, 'colsample_bytree': 0.7307749963219472, 'lambda': 0.1697008050569727, 'alpha': 0.00045746225925563064, 'learning_rate': 0.21288125193848467}\n",
      "\n",
      "--- Training Modello Finale Ottimizzato ---\n",
      "\n",
      "--- RISULTATI FINALI SUL TEST SET ---\n",
      "1. AUC Score Baseline:    0.99008\n",
      "2. AUC Score Ottimizzato: 0.99405\n",
      "------------------------------\n",
      "Miglioramento AUC:        +0.00397\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "# Silenziamo alcuni warning di Optuna per pulizia\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# 1. Preparazione Dati\n",
    "# ==========================================\n",
    "print(\"--- Caricamento Dati ---\")\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split 1: Togliamo il 20% dei dati per il TEST finale (Dati mai visti né da XGBoost né da Optuna)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Calcolo scale_pos_weight per bilanciamento (best practice)\n",
    "# Anche se questo dataset è abbastanza bilanciato, lo calcoliamo per rigore professionale\n",
    "num_neg = np.sum(y_train_full == 0)\n",
    "num_pos = np.sum(y_train_full == 1)\n",
    "scale_pos_weight = num_neg / num_pos\n",
    "\n",
    "print(f\"Dataset Shape: {X.shape}\")\n",
    "print(f\"Scale Pos Weight calcolato: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Baseline (Senza Tuning)\n",
    "# ==========================================\n",
    "print(\"\\n--- Training Modello Baseline (Default) ---\")\n",
    "dtrain_full = xgb.DMatrix(X_train_full, label=y_train_full)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Parametri standard\n",
    "params_base = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'tree_method': 'hist', # Più veloce\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "model_base = xgb.train(params_base, dtrain_full, num_boost_round=100)\n",
    "preds_base = model_base.predict(dtest)\n",
    "auc_base = roc_auc_score(y_test, preds_base)\n",
    "print(f\"Baseline AUC Score: {auc_base:.4f}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. Ottimizzazione con Optuna\n",
    "# ==========================================\n",
    "print(\"\\n--- Inizio Tuning con Optuna ---\")\n",
    "\n",
    "def objective(trial):\n",
    "    # A. Split interno per Optuna (Train vs Validation)\n",
    "    # Optuna usa questo valid set per decidere se i parametri sono buoni\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "    # B. Definizione Spazio Iperparametri\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss', # Metrica monitorata per il pruning\n",
    "        'tree_method': 'hist',\n",
    "        'scale_pos_weight': scale_pos_weight,\n",
    "        \n",
    "        # --- Parametri da Ottimizzare ---\n",
    "        # 1. Struttura\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        \n",
    "        # 2. Campionamento\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        \n",
    "        # 3. Regolarizzazione\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True),\n",
    "        \n",
    "        # 4. Learning\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "    }\n",
    "\n",
    "    # Callback per il Pruning (Interrompe i trial scarsi)\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-logloss\")\n",
    "\n",
    "    # Training del trial\n",
    "    model = xgb.train(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=1000, # Alto numero teorico\n",
    "        evals=[(dvalid, \"validation\")],\n",
    "        early_stopping_rounds=50, # Stop se non migliora\n",
    "        callbacks=[pruning_callback],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Predizione sul validation set interno\n",
    "    preds = model.predict(dvalid)\n",
    "    # Ottimizziamo la LogLoss (più bassa è meglio)\n",
    "    # Nota: Optuna minimizza per default, log_loss è perfetta\n",
    "    loss = classification_report(y_valid, preds > 0.5, output_dict=True)['accuracy'] # Trucco: ottimizziamo accuracy invertita o logloss diretta\n",
    "    \n",
    "    # Per semplicità in questo esempio usiamo logloss diretta\n",
    "    # Importante: model.predict restituisce probabilità\n",
    "    from sklearn.metrics import log_loss\n",
    "    return log_loss(y_valid, preds)\n",
    "\n",
    "# Creazione Studio\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, timeout=600) # 50 tentativi o 10 minuti\n",
    "\n",
    "print(\"Migliori parametri trovati:\")\n",
    "print(study.best_params)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. Training Modello Finale (Best Params)\n",
    "# ==========================================\n",
    "print(\"\\n--- Training Modello Finale Ottimizzato ---\")\n",
    "\n",
    "# Recuperiamo i migliori parametri\n",
    "best_params = study.best_params\n",
    "\n",
    "# Aggiungiamo i parametri fissi necessari\n",
    "best_params['objective'] = 'binary:logistic'\n",
    "best_params['eval_metric'] = 'logloss'\n",
    "best_params['tree_method'] = 'hist'\n",
    "best_params['scale_pos_weight'] = scale_pos_weight\n",
    "\n",
    "# TECNICA DEL \"LOW LEARNING RATE\" (Shrinkage)\n",
    "# Riduciamo il learning rate trovato e aumentiamo gli alberi per precisione massima\n",
    "best_params['learning_rate'] = best_params['learning_rate'] / 2 \n",
    "num_boost_round_final = 2000 \n",
    "\n",
    "# Addestriamo su TUTTO il set di training (Train + Valid interno di Optuna)\n",
    "model_opt = xgb.train(\n",
    "    best_params, \n",
    "    dtrain_full, \n",
    "    num_boost_round=num_boost_round_final,\n",
    "    evals=[(dtest, \"test\")], # Usiamo il test set solo per early stopping finale\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 5. Confronto Finale\n",
    "# ==========================================\n",
    "print(\"\\n--- RISULTATI FINALI SUL TEST SET ---\")\n",
    "\n",
    "# Predizioni\n",
    "preds_opt = model_opt.predict(dtest)\n",
    "\n",
    "# Metriche\n",
    "auc_opt = roc_auc_score(y_test, preds_opt)\n",
    "acc_base = accuracy_score(y_test, preds_base > 0.5)\n",
    "acc_opt = accuracy_score(y_test, preds_opt > 0.5)\n",
    "\n",
    "print(f\"1. AUC Score Baseline:    {auc_base:.5f}\")\n",
    "print(f\"2. AUC Score Ottimizzato: {auc_opt:.5f}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Miglioramento AUC:        {auc_opt - auc_base:+.5f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Visualizzazione importanza iperparametri (se in notebook)\n",
    "# optuna.visualization.plot_param_importances(study).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2566963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (20640, 8), Target medio: 2.07\n",
      "Baseline RMSE: 0.4581\n",
      "\n",
      "Best validation RMSE: 0.4612579778244934\n",
      "Best parameters: {'max_depth': 7, 'min_child_weight': 5, 'gamma': 1.7700023226065948e-05, 'subsample': 0.673502300873134, 'colsample_bytree': 0.7857990792075301, 'lambda': 4.5475339914832475e-07, 'alpha': 3.922347665504824e-08, 'learning_rate': 0.0812110716460948, 'max_bin': 418, 'grow_policy': 'depthwise'}\n",
      "\n",
      "--- Confronto finale ---\n",
      "RMSE Baseline:    0.4581\n",
      "RMSE Ottimizzato: 0.4380\n",
      "--> Miglioramento Errore: 0.0201\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Pulizia output\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===========================\n",
    "# 1️⃣ Caricamento dati\n",
    "# ===========================\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame\n",
    "X = df.drop(columns=['MedHouseVal']).values\n",
    "y = df['MedHouseVal'].values\n",
    "\n",
    "# Log-transform target per stabilità numerica\n",
    "y = np.log1p(y)\n",
    "\n",
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train/test split finale\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}, Target medio: {np.mean(np.expm1(y)):.2f}\")\n",
    "\n",
    "# ===========================\n",
    "# 2️⃣ Baseline CPU (per confronto)\n",
    "# ===========================\n",
    "dtrain_full = xgb.DMatrix(X_train_full, label=y_train_full)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params_base = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'hist',\n",
    "    'random_state': 42,\n",
    "    'max_depth': 8,\n",
    "    'eta': 0.1\n",
    "}\n",
    "\n",
    "model_base = xgb.train(params_base, dtrain_full, num_boost_round=200)\n",
    "preds_base = np.expm1(model_base.predict(dtest))\n",
    "rmse_base = np.sqrt(mean_squared_error(np.expm1(y_test), preds_base))\n",
    "print(f\"Baseline RMSE: {rmse_base:.4f}\")\n",
    "\n",
    "# ===========================\n",
    "# 3️⃣ Funzione Objective per Optuna (GPU)\n",
    "# ===========================\n",
    "def objective(trial):\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    rmses = []\n",
    "\n",
    "    for train_idx, valid_idx in kf.split(X_train_full):\n",
    "        X_train, X_valid = X_train_full[train_idx], X_train_full[valid_idx]\n",
    "        y_train, y_valid = y_train_full[train_idx], y_train_full[valid_idx]\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "        param = {\n",
    "            'verbosity': 0,\n",
    "            'objective': 'reg:squarederror',\n",
    "            'eval_metric': 'rmse',\n",
    "\n",
    "            # GPU\n",
    "            'tree_method': 'gpu_hist',\n",
    "            'predictor': 'gpu_predictor',\n",
    "\n",
    "            # Alberi\n",
    "            'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "            'min_child_weight': trial.suggest_int('min_child_weight', 1, 15),\n",
    "            'gamma': trial.suggest_float('gamma', 1e-8, 5.0, log=True),\n",
    "\n",
    "            # Randomness\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "\n",
    "            # Regolarizzazione\n",
    "            'lambda': trial.suggest_float('lambda', 1e-8, 50.0, log=True),\n",
    "            'alpha': trial.suggest_float('alpha', 1e-8, 50.0, log=True),\n",
    "\n",
    "            # Learning\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            'max_bin': trial.suggest_int('max_bin', 128, 512),\n",
    "\n",
    "            'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "            'booster': 'gbtree'\n",
    "        }\n",
    "\n",
    "        pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\")\n",
    "\n",
    "        model = xgb.train(\n",
    "            param,\n",
    "            dtrain,\n",
    "            num_boost_round=1000,\n",
    "            evals=[(dvalid, \"validation\")],\n",
    "            early_stopping_rounds=50,\n",
    "            callbacks=[pruning_callback],\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        preds = model.predict(dvalid)\n",
    "        rmse = np.sqrt(mean_squared_error(np.expm1(y_valid), np.expm1(preds)))\n",
    "        rmses.append(rmse)\n",
    "\n",
    "    return np.mean(rmses)\n",
    "\n",
    "# ===========================\n",
    "# 4️⃣ Run Optuna\n",
    "# ===========================\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, timeout=600)  # 50 trial max 10 minuti\n",
    "\n",
    "print(\"\\nBest validation RMSE:\", study.best_value)\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "\n",
    "# ===========================\n",
    "# 5️⃣ Training modello finale GPU\n",
    "# ===========================\n",
    "best_params = study.best_params.copy()\n",
    "best_params.update({\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'predictor': 'gpu_predictor'\n",
    "})\n",
    "\n",
    "# Riduzione learning rate finale per \"shrinkage\"\n",
    "best_params['learning_rate'] *= 0.5\n",
    "num_round_final = 8000\n",
    "\n",
    "dtrain_f = xgb.DMatrix(X_train_full, label=y_train_full)\n",
    "dtest_f = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "model_opt = xgb.train(\n",
    "    best_params,\n",
    "    dtrain_f,\n",
    "    num_boost_round=num_round_final,\n",
    "    evals=[(dtest_f, \"validation\")],\n",
    "    early_stopping_rounds=200,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# 6️⃣ Valutazione finale\n",
    "# ===========================\n",
    "preds_opt = np.expm1(model_opt.predict(dtest_f))\n",
    "rmse_opt = np.sqrt(mean_squared_error(np.expm1(y_test), preds_opt))\n",
    "\n",
    "print(\"\\n--- Confronto finale ---\")\n",
    "print(f\"RMSE Baseline:    {rmse_base:.4f}\")\n",
    "print(f\"RMSE Ottimizzato: {rmse_opt:.4f}\")\n",
    "print(f\"--> Miglioramento Errore: {rmse_base - rmse_opt:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e2c610",
   "metadata": {},
   "source": [
    "--- Caricamento California Housing Dataset ---\n",
    "Dataset Shape: (20640, 8)\n",
    "Target medio: 2.07 (centinaia di k$)\n",
    "\n",
    "--- Training Modello Baseline (Default) ---\n",
    "Baseline RMSE: 0.4718\n",
    "\n",
    "--- Inizio Tuning con Optuna ---\n",
    "\n",
    "Miglior RMSE trovato da Optuna (Validation): 0.4667\n",
    "Migliori parametri: {'max_depth': 6, 'min_child_weight': 1, 'gamma': 2.647729432344551e-06, 'subsample': 0.7057560887773185, 'colsample_bytree': 0.8261113044843484, 'lambda': 0.1073402821501809, 'alpha': 0.12334515017305431, 'learning_rate': 0.037390927995818786}\n",
    "\n",
    "--- Training Modello Finale Ottimizzato ---\n",
    "\n",
    "--- CONFRONTO FINALE ---\n",
    "RMSE Baseline:    0.4718\n",
    "RMSE Ottimizzato: 0.4362\n",
    "--> Miglioramento Errore: 0.0356 (Minore è meglio)\n",
    "------------------------------\n",
    "R2 Score Baseline:    0.8301\n",
    "R2 Score Ottimizzato: 0.8548\n",
    "--> Varianza Spiegata Extra: +2.47%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234b187",
   "metadata": {},
   "source": [
    "### **Pro-Tip: Monotonic Constraints su questo Dataset**\n",
    "\n",
    "Se volessi rendere questo modello \"bullet-proof\" per un'agenzia immobiliare reale, dovresti aggiungere un vincolo al parametro `AveRooms` (Numero medio di stanze).\n",
    "\n",
    "Nella realtà, *a parità di altre condizioni* (stessa zona, stessa età), una casa con più stanze vale di più. Se il modello imparasse il contrario (magari per rumore statistico), sarebbe un errore grave.\n",
    "\n",
    "Potresti aggiungere questo nel `param` finale:\n",
    "\n",
    "```python\n",
    "# Supponendo che 'AveRooms' sia la colonna indice 2\n",
    "# 1 = crescente, 0 = nessun vincolo\n",
    "# Imponiamo che più stanze = prezzo più alto (o uguale)\n",
    "constraints = (0, 0, 1, 0, 0, 0, 0, 0) \n",
    "best_params['monotone_constraints'] = constraints\n",
    "```\n",
    "\n",
    "Questo peggiorerebbe leggermente l'RMSE matematico, ma renderebbe il modello infinitamente più affidabile nel mondo reale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
