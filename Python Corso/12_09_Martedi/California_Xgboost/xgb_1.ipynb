{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da433176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: train (16512, 11) test (4128, 11)\n",
      "Training ensemble for uncertainty estimation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-09 16:45:55,856] A new study created in memory with name: no-name-6ea733b0-5079-42b0-b0d6-cb0043001b6e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble-base mean RMSE: 0.4317, R2: 0.8576\n",
      "Selected 1032 pseudo-labels (lowest uncertainty)\n",
      "After range filter: 1032 pseudo-labels kept\n",
      "Combined training shape: (17544, 11)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd58c727d8544c808eea92edde762d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-12-09 16:46:00,210] Trial 0 finished with value: 0.4582714444845233 and parameters: {'max_depth': 13, 'min_child_weight': 2, 'subsample': 0.8230824575171154, 'colsample_bytree': 0.7839284218668187, 'eta': 0.029943869879060125, 'gamma': 0.28879669856801526, 'lambda': 0.7292464707684743, 'alpha': 9.439934601355491e-05}. Best is trial 0 with value: 0.4582714444845233.\n",
      "[I 2025-12-09 16:46:09,581] Trial 1 finished with value: 0.44250239616179254 and parameters: {'max_depth': 6, 'min_child_weight': 19, 'subsample': 0.6947389816235756, 'colsample_bytree': 0.6221244775481586, 'eta': 0.017189859118766165, 'gamma': 0.00027292220105410266, 'lambda': 3.078306250593259e-06, 'alpha': 9.925905435678324e-05}. Best is trial 1 with value: 0.44250239616179254.\n",
      "[I 2025-12-09 16:46:26,748] Trial 2 finished with value: 0.4523786896289224 and parameters: {'max_depth': 13, 'min_child_weight': 11, 'subsample': 0.9416011491529339, 'colsample_bytree': 0.8157178372337522, 'eta': 0.021785040848999543, 'gamma': 2.6733909487003385e-05, 'lambda': 0.00025283637697113545, 'alpha': 0.0009915390235885207}. Best is trial 1 with value: 0.44250239616179254.\n",
      "[I 2025-12-09 16:46:31,166] Trial 3 finished with value: 0.45495380033654953 and parameters: {'max_depth': 12, 'min_child_weight': 11, 'subsample': 0.7821155836986893, 'colsample_bytree': 0.7850098617798926, 'eta': 0.0302934180483763, 'gamma': 0.4519785844805949, 'lambda': 0.00020197336555037026, 'alpha': 0.00846700720474197}. Best is trial 1 with value: 0.44250239616179254.\n",
      "[I 2025-12-09 16:46:41,248] Trial 4 finished with value: 0.4438574756056978 and parameters: {'max_depth': 9, 'min_child_weight': 18, 'subsample': 0.6407868271778442, 'colsample_bytree': 0.7143988000428281, 'eta': 0.01991345142640107, 'gamma': 0.0007425461186727354, 'lambda': 0.7663487059675996, 'alpha': 0.050017305612436024}. Best is trial 1 with value: 0.44250239616179254.\n",
      "[I 2025-12-09 16:46:45,528] Trial 5 finished with value: 0.45372428841720847 and parameters: {'max_depth': 9, 'min_child_weight': 3, 'subsample': 0.9482827765916608, 'colsample_bytree': 0.7745546466410699, 'eta': 0.06353551076248606, 'gamma': 0.005961710522614319, 'lambda': 0.0007170361557119076, 'alpha': 0.009931136148769044}. Best is trial 1 with value: 0.44250239616179254.\n",
      "[I 2025-12-09 16:47:02,650] Trial 6 finished with value: 0.4498293886682532 and parameters: {'max_depth': 13, 'min_child_weight': 4, 'subsample': 0.6865852766459417, 'colsample_bytree': 0.6637555558720872, 'eta': 0.03998949998380021, 'gamma': 0.00013026023288583563, 'lambda': 0.0047208838971465415, 'alpha': 2.3282517848739294e-05}. Best is trial 1 with value: 0.44250239616179254.\n",
      "[I 2025-12-09 16:47:05,830] Trial 7 finished with value: 0.4501353391536043 and parameters: {'max_depth': 9, 'min_child_weight': 10, 'subsample': 0.9229745024379077, 'colsample_bytree': 0.7701071862628135, 'eta': 0.08494662291165823, 'gamma': 8.393975939312508e-07, 'lambda': 0.006012520312053962, 'alpha': 0.0006787356026949709}. Best is trial 1 with value: 0.44250239616179254.\n",
      "[I 2025-12-09 16:47:07,794] Trial 8 finished with value: 0.45764859581697165 and parameters: {'max_depth': 8, 'min_child_weight': 14, 'subsample': 0.7767255615729197, 'colsample_bytree': 0.8314788091721148, 'eta': 0.09877341122637497, 'gamma': 0.3909242648696589, 'lambda': 1.914678160045659e-06, 'alpha': 1.6172317816604133e-06}. Best is trial 1 with value: 0.44250239616179254.\n",
      "[I 2025-12-09 16:47:15,476] Trial 9 finished with value: 0.4402290046461942 and parameters: {'max_depth': 9, 'min_child_weight': 16, 'subsample': 0.9306477861761704, 'colsample_bytree': 0.7220967347664454, 'eta': 0.022669574305904532, 'gamma': 0.15312509955882245, 'lambda': 3.100562954524202, 'alpha': 6.005179692795665e-05}. Best is trial 9 with value: 0.4402290046461942.\n",
      "[I 2025-12-09 16:47:36,636] Trial 10 finished with value: 0.4424179923790859 and parameters: {'max_depth': 11, 'min_child_weight': 15, 'subsample': 0.8493506286060686, 'colsample_bytree': 0.9901823323595704, 'eta': 0.011070638728461316, 'gamma': 1.13401469015485e-08, 'lambda': 7.007162617295376, 'alpha': 3.3696116641451797}. Best is trial 9 with value: 0.4402290046461942.\n",
      "[I 2025-12-09 16:47:58,673] Trial 11 finished with value: 0.442005607377879 and parameters: {'max_depth': 11, 'min_child_weight': 15, 'subsample': 0.8602874945195373, 'colsample_bytree': 0.9852471172636614, 'eta': 0.010009162789983397, 'gamma': 2.485249594943736e-08, 'lambda': 8.840553206517756, 'alpha': 1.3128226968727013}. Best is trial 9 with value: 0.4402290046461942.\n",
      "[I 2025-12-09 16:48:27,556] Trial 12 finished with value: 0.44370599456597026 and parameters: {'max_depth': 11, 'min_child_weight': 16, 'subsample': 0.8744105316162418, 'colsample_bytree': 0.9574439187285365, 'eta': 0.010304193267233412, 'gamma': 2.204653327863773e-08, 'lambda': 9.165597529961973, 'alpha': 4.225728248462922}. Best is trial 9 with value: 0.4402290046461942.\n",
      "[I 2025-12-09 16:49:01,625] Trial 13 finished with value: 0.4497560636744363 and parameters: {'max_depth': 11, 'min_child_weight': 7, 'subsample': 0.9953160555104112, 'colsample_bytree': 0.8961183702192127, 'eta': 0.013325938925093414, 'gamma': 9.416009226621671e-07, 'lambda': 0.11226996557528833, 'alpha': 0.1407670292868366}. Best is trial 9 with value: 0.4402290046461942.\n",
      "[I 2025-12-09 16:49:07,172] Trial 14 finished with value: 0.44724390484177556 and parameters: {'max_depth': 7, 'min_child_weight': 20, 'subsample': 0.8914377557796244, 'colsample_bytree': 0.898660460222737, 'eta': 0.04435440763412417, 'gamma': 0.011567908781322826, 'lambda': 0.07578847020424899, 'alpha': 0.5056723209158682}. Best is trial 9 with value: 0.4402290046461942.\n",
      "[W 2025-12-09 16:49:11,000] Trial 15 failed with parameters: {'max_depth': 10, 'min_child_weight': 13, 'subsample': 0.7532366150223767, 'colsample_bytree': 0.722730847042813, 'eta': 0.015372480338642821, 'gamma': 7.241520581029206e-06, 'lambda': 1.1061198726372585, 'alpha': 1.5259981349520727e-06} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\39392\\AppData\\Local\\Temp\\ipykernel_25288\\1216310574.py\", line 171, in objective_xgb\n",
      "    model = try_train_xgb(params, dtrain, dval, num_round=5000, early_stopping=120, verbose=False)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\39392\\AppData\\Local\\Temp\\ipykernel_25288\\1216310574.py\", line 70, in try_train_xgb\n",
      "    return xgb.train(params, dtrain, num_boost_round=num_round,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\training.py\", line 186, in train\n",
      "    if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\callback.py\", line 240, in after_iteration\n",
      "    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Python312\\Lib\\site-packages\\xgboost\\core.py\", line 1989, in eval_set\n",
      "    _LIB.XGBoosterEvalOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-12-09 16:49:11,003] Trial 15 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 177\u001b[39m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(np.sqrt(mean_squared_error(y_val, preds)))\n\u001b[32m    176\u001b[39m study_xgb = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[43mstudy_xgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_xgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS_XGB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest XGB params (raw):\u001b[39m\u001b[33m\"\u001b[39m, study_xgb.best_params)\n\u001b[32m    180\u001b[39m best_xgb = study_xgb.best_params.copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\optuna\\study\\study.py:490\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    389\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    390\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    398\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    488\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:67\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:164\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    161\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     frozen_trial_id = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:262\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    258\u001b[39m     updated_state == TrialState.FAIL\n\u001b[32m    259\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    261\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trial._trial_id\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\optuna\\study\\_optimize.py:205\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    207\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    208\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 171\u001b[39m, in \u001b[36mobjective_xgb\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    153\u001b[39m params = {\n\u001b[32m    154\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m    155\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mreg:squarederror\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m     \u001b[33m'\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m'\u001b[39m: trial.suggest_float(\u001b[33m'\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1e-6\u001b[39m, \u001b[32m10.0\u001b[39m, log=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    168\u001b[39m }\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# try to train; if GPU errors, fallback inside try_train_xgb\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m model = \u001b[43mtry_train_xgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_round\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m preds = model.predict(dval)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(np.sqrt(mean_squared_error(y_val, preds)))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mtry_train_xgb\u001b[39m\u001b[34m(params, dtrain, dval, num_round, early_stopping, verbose)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtry_train_xgb\u001b[39m(params, dtrain, dval, num_round=\u001b[32m500\u001b[39m, early_stopping=\u001b[32m30\u001b[39m, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxgb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_round\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m XGBoostError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     74\u001b[39m         \u001b[38;5;66;03m# fallback to CPU hist if GPU errors\u001b[39;00m\n\u001b[32m     75\u001b[39m         params_f = params.copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\xgboost\\core.py:620\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    618\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    619\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\xgboost\\training.py:186\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    184\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    185\u001b[39m     bst.update(dtrain, i, obj)\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb_container\u001b[49m\u001b[43m.\u001b[49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    187\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    189\u001b[39m bst = cb_container.after_training(bst)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\xgboost\\callback.py:240\u001b[39m, in \u001b[36mCallbackContainer.after_iteration\u001b[39m\u001b[34m(self, model, epoch, dtrain, evals)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, name \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m name.find(\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m) == -\u001b[32m1\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDataset name should not contain `-`\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m score: \u001b[38;5;28mstr\u001b[39m = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_output_margin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m splited = score.split()[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# into datasets\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# split up `test-error:0.1234`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\xgboost\\core.py:1989\u001b[39m, in \u001b[36mBooster.eval_set\u001b[39m\u001b[34m(self, evals, iteration, feval, output_margin)\u001b[39m\n\u001b[32m   1986\u001b[39m evnames = c_array(ctypes.c_char_p, [c_str(d[\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m evals])\n\u001b[32m   1987\u001b[39m msg = ctypes.c_char_p()\n\u001b[32m   1988\u001b[39m _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m1989\u001b[39m     \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterEvalOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1990\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1992\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdmats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1993\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1994\u001b[39m \u001b[43m        \u001b[49m\u001b[43mc_bst_ulong\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1995\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1996\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1997\u001b[39m )\n\u001b[32m   1998\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m msg.value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1999\u001b[39m res = msg.value.decode()  \u001b[38;5;66;03m# pylint: disable=no-member\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==========================================\n",
    "# 1. Importazione e preprocessing\n",
    "# ==========================================\n",
    "raw_data = fetch_california_housing()\n",
    "data = pd.DataFrame(raw_data.data, columns=raw_data.feature_names)\n",
    "data['MedHouseVal'] = raw_data.target\n",
    "\n",
    "# Rimozione outlier\n",
    "numeric_cols = data.select_dtypes(include=[\"float\", \"int\"]).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.756)\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "    data[col] = np.where(data[col].between(lower, upper), data[col], np.nan)\n",
    "data = data.dropna()\n",
    "\n",
    "# Feature combinate\n",
    "data['Ave_Room_Bed'] = data['AveRooms'] / data['AveBedrms']\n",
    "data['HouseRooms'] = data['HouseAge'] / data['AveRooms']\n",
    "data['OccupPerPerson'] = data['AveOccup'] / (data['Population'] + 1e-5)\n",
    "data['RoomsPerPerson'] = data['AveRooms'] / (data['Population'] + 1e-5)\n",
    "data['BedroomsPerPerson'] = data['AveBedrms'] / (data['Population'] + 1e-5)\n",
    "data['Lat_Long'] = data['Latitude'] * data['Longitude']\n",
    "\n",
    "# ==========================================\n",
    "# 2. Split e scaling\n",
    "# ==========================================\n",
    "X = data.drop(columns=[\"MedHouseVal\"])\n",
    "y = data[\"MedHouseVal\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Baseline XGBoost\n",
    "# ==========================================\n",
    "dtrain = xgb.DMatrix(X_train_scaled, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test_scaled, label=y_test)\n",
    "\n",
    "params_base = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'predictor': 'gpu_predictor',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "model_base = xgb.train(params_base, dtrain, num_boost_round=100)\n",
    "preds_base = model_base.predict(dtest)\n",
    "rmse_base = np.sqrt(mean_squared_error(y_test, preds_base))\n",
    "r2_base = r2_score(y_test, preds_base)\n",
    "print(f\"RMSE Baseline XGBoost: {rmse_base:.4f}, R2: {r2_base:.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Pseudo-labeling\n",
    "# ==========================================\n",
    "preds_test = model_base.predict(dtest)\n",
    "confident_idx = np.argsort(np.abs(preds_test - np.mean(preds_test)))[:int(0.25*len(preds_test))]\n",
    "\n",
    "X_pseudo = X_test_scaled[confident_idx]\n",
    "y_pseudo = preds_test[confident_idx]\n",
    "\n",
    "X_combined = np.vstack([X_train_scaled, X_pseudo])\n",
    "y_combined = np.hstack([y_train.values, y_pseudo])\n",
    "\n",
    "print(f\"Training con pseudo-label: {X_combined.shape[0]} campioni\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. Hyperparameter tuning XGBoost\n",
    "# ==========================================\n",
    "def objective_xgb(trial):\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "    dtrain_trial = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dvalid_trial = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'predictor': 'gpu_predictor',\n",
    "        'max_depth': trial.suggest_int('max_depth', 6, 14),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 25),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-7, 1.5, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-7, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-7, 10.0, log=True),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2)\n",
    "    }\n",
    "    \n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\")\n",
    "    model = xgb.train(param, dtrain_trial, num_boost_round=2000, evals=[(dvalid_trial, \"validation\")],\n",
    "                      early_stopping_rounds=100, callbacks=[pruning_callback], verbose_eval=False)\n",
    "    \n",
    "    preds_val = model.predict(dvalid_trial)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
    "    return rmse\n",
    "\n",
    "study_xgb = optuna.create_study(direction='minimize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=50)\n",
    "\n",
    "best_params_xgb = study_xgb.best_params\n",
    "best_params_xgb.update({\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'predictor': 'gpu_predictor'\n",
    "})\n",
    "\n",
    "best_params_xgb['learning_rate'] *= 0.5\n",
    "model_xgb_final = xgb.train(best_params_xgb, xgb.DMatrix(X_combined, label=y_combined),\n",
    "                            num_boost_round=5000, evals=[(dtest, \"test\")],\n",
    "                            early_stopping_rounds=100, verbose_eval=False)\n",
    "\n",
    "preds_xgb_final = model_xgb_final.predict(dtest)\n",
    "rmse_xgb_final = np.sqrt(mean_squared_error(y_test, preds_xgb_final))\n",
    "r2_xgb_final = r2_score(y_test, preds_xgb_final)\n",
    "print(f\"XGBoost finale pseudo-label: RMSE={rmse_xgb_final:.4f}, R2={r2_xgb_final:.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. Hyperparameter tuning LightGBM\n",
    "# ==========================================\n",
    "def objective_lgbm(trial):\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "    \n",
    "    param = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50)\n",
    "    }\n",
    "    \n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        param,\n",
    "        train_data,\n",
    "        num_boost_round=5000,\n",
    "        valid_sets=[valid_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    preds_val = model.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds_val))\n",
    "    return rmse\n",
    "\n",
    "study_lgbm = optuna.create_study(direction='minimize')\n",
    "study_lgbm.optimize(objective_lgbm, n_trials=50)\n",
    "\n",
    "best_params_lgbm = study_lgbm.best_params\n",
    "best_params_lgbm.update({'objective':'regression','metric':'rmse','verbosity':-1})\n",
    "\n",
    "# Training finale LGBM\n",
    "train_lgbm_final = lgb.Dataset(X_combined, label=y_combined)\n",
    "valid_lgbm = lgb.Dataset(X_test_scaled, label=y_test, reference=train_lgbm_final)\n",
    "\n",
    "model_lgbm_final = lgb.train(\n",
    "    best_params_lgbm,\n",
    "    train_lgbm_final,\n",
    "    num_boost_round=5000,\n",
    "    valid_sets=[valid_lgbm],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "        lgb.log_evaluation(0)\n",
    "    ]\n",
    ")\n",
    "\n",
    "preds_lgbm_final = model_lgbm_final.predict(X_test_scaled)\n",
    "rmse_lgbm_final = np.sqrt(mean_squared_error(y_test, preds_lgbm_final))\n",
    "r2_lgbm_final = r2_score(y_test, preds_lgbm_final)\n",
    "print(f\"LightGBM finale pseudo-label: RMSE={rmse_lgbm_final:.4f}, R2={r2_lgbm_final:.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 7. Confronto finale\n",
    "# ==========================================\n",
    "print(\"-\"*40)\n",
    "print(f\"Baseline XGBoost: RMSE={rmse_base:.4f}, R2={r2_base:.4f}\")\n",
    "print(f\"XGBoost finale:   RMSE={rmse_xgb_final:.4f}, R2={r2_xgb_final:.4f}\")\n",
    "print(f\"LightGBM finale:  RMSE={rmse_lgbm_final:.4f}, R2={r2_lgbm_final:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d05c62",
   "metadata": {},
   "source": [
    "**RISULTATO**\n",
    "\n",
    "RMSE Baseline:    0.4718\n",
    "RMSE Ottimizzato: 0.4300\n",
    "\n",
    "param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse', # Metrica per early stopping e pruning\n",
    "        'tree_method': 'hist',\n",
    "        \n",
    "        # --- Struttura ---\n",
    "        # California ha interazioni complesse, permettiamo alberi più profondi\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 15),\n",
    "        # Importante per evitare overfitting su outlier di prezzo\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        \n",
    "        # --- Randomness ---\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        \n",
    "        # --- Regolarizzazione ---\n",
    "        # Fondamentale nella regressione per non inseguire i prezzi estremi\n",
    "        'lambda': trial.suggest_float('lambda', 1e-9, 25.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-9, 25.0, log=True),\n",
    "        \n",
    "        # --- Learning ---\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "    }\n",
    "\n",
    "study.optimize(objective, n_trials=70, timeout=600)\n",
    "\n",
    "=============================================================================== <br>\n",
    "RMSE Baseline:    0.4718\n",
    "RMSE Ottimizzato: 0.4310\n",
    "\n",
    "con scaler\n",
    "\n",
    "param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse', # Metrica per early stopping e pruning\n",
    "        'tree_method': 'hist',\n",
    "        \n",
    "        # --- Struttura ---\n",
    "        # California ha interazioni complesse, permettiamo alberi più profondi\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
    "        # Importante per evitare overfitting su outlier di prezzo\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 25),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-9, 1.5, log=True),\n",
    "        \n",
    "        # --- Randomness ---\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        \n",
    "        # --- Regolarizzazione ---\n",
    "        # Fondamentale nella regressione per non inseguire i prezzi estremi\n",
    "        'lambda': trial.suggest_float('lambda', 1e-6, 20.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-6, 20.0, log=True),\n",
    "        \n",
    "        # --- Learning ---\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.8),\n",
    "    }\n",
    "\n",
    "study.optimize(objective, n_trials=200, timeout=600)\n",
    "\n",
    "=============================================================================== <br>\n",
    "\n",
    "RMSE Baseline:    0.3522\n",
    "RMSE Ottimizzato: 0.3379\n",
    "\n",
    "con scaler\n",
    "\n",
    "study.optimize(objective, n_trials=200, timeout=600)\n",
    "\n",
    "param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse', # Metrica per early stopping e pruning\n",
    "        'tree_method': 'hist',\n",
    "        \n",
    "        # --- Struttura ---\n",
    "        # California ha interazioni complesse, permettiamo alberi più profondi\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
    "        # Importante per evitare overfitting su outlier di prezzo\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 25),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-7, 1.5, log=True),\n",
    "        \n",
    "        # --- Randomness ---\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        \n",
    "        # --- Regolarizzazione ---\n",
    "        # Fondamentale nella regressione per non inseguire i prezzi estremi\n",
    "        'lambda': trial.suggest_float('lambda', 1e-9, 15.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-9, 15.0, log=True),\n",
    "        \n",
    "        # --- Learning ---\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.6),\n",
    "    }\n",
    "\n",
    "#rimozione outlier\n",
    "numeric_cols = data.select_dtypes(include=[\"float\", \"int\"]).columns.tolist()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = data[col].quantile(0.3)\n",
    "    Q3 = data[col].quantile(0.7)\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "\n",
    "    data[col] = np.where(data[col].between(lower, upper), data[col], np.nan)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "#features combinate\n",
    "data['Ave_Room_Bed'] = data['AveRooms'] / data['AveBedrms']\n",
    "data['HouseRooms'] = data['HouseAge'] / data['AveRooms']\n",
    "\n",
    "Miglior RMSE trovato da Optuna (Validation): 0.3544\n",
    "Migliori parametri: {'max_depth': 13, 'min_child_weight': 23, 'gamma': 3.8838420870962745e-06, 'subsample': 0.7570628531701007, 'colsample_bytree': 0.9109736816364161, 'lambda': 0.02307771384295654, 'alpha': 0.9454327224813569, 'learning_rate': 0.06885420667941514}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
