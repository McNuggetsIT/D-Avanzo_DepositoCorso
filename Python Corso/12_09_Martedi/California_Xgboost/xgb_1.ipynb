{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da433176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   MedInc       20640 non-null  float64\n",
      " 1   HouseAge     20640 non-null  float64\n",
      " 2   AveRooms     20640 non-null  float64\n",
      " 3   AveBedrms    20640 non-null  float64\n",
      " 4   Population   20640 non-null  float64\n",
      " 5   AveOccup     20640 non-null  float64\n",
      " 6   Latitude     20640 non-null  float64\n",
      " 7   Longitude    20640 non-null  float64\n",
      " 8   MedHouseVal  20640 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import warnings\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "#importazione dei dataset\n",
    "raw_data = fetch_california_housing()\n",
    "data = pd.DataFrame(raw_data.data, columns=raw_data.feature_names)\n",
    "data['MedHouseVal'] = raw_data.target\n",
    "\n",
    "data.info()\n",
    "\n",
    "#rimozione outlier\n",
    "numeric_cols = data.select_dtypes(include=[\"float\", \"int\"]).columns.tolist()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = data[col].quantile(0.3)\n",
    "    Q3 = data[col].quantile(0.7)\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "\n",
    "    data[col] = np.where(data[col].between(lower, upper), data[col], np.nan)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "#features combinate\n",
    "data['Ave_Room_Bed'] = data['AveRooms'] / data['AveBedrms']\n",
    "data['HouseRooms'] = data['HouseAge'] / data['AveRooms']\n",
    "data['OccupPerPerson'] = data['AveOccup'] / (data['Population'] + 1e-5)\n",
    "data['RoomsPerPerson'] = data['AveRooms'] / (data['Population'] + 1e-5)\n",
    "data['BedroomsPerPerson'] = data['AveBedrms'] / (data['Population'] + 1e-5)\n",
    "data['Lat_Long'] = data['Latitude'] * data['Longitude']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6118310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Split interno per validazione durante il tuning\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "    # Spazio di ricerca\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse', # Metrica per early stopping e pruning\n",
    "\n",
    "        'tree_method': 'gpu_hist',   # Usa il metodo basato su GPU\n",
    "        'predictor': 'gpu_predictor', # Predizioni anch’esse su GPU\n",
    "        \n",
    "        # --- Struttura ---\n",
    "        # California ha interazioni complesse, permettiamo alberi più profondi\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
    "        # Importante per evitare overfitting su outlier di prezzo\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 25),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-7, 1.5, log=True),\n",
    "        \n",
    "        # --- Randomness ---\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n",
    "        \n",
    "        # --- Regolarizzazione ---\n",
    "        # Fondamentale nella regressione per non inseguire i prezzi estremi\n",
    "        'lambda': trial.suggest_float('lambda', 1e-9, 15.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-9, 15.0, log=True),\n",
    "        \n",
    "        # --- Learning ---\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 1),\n",
    "    }\n",
    "\n",
    "    # Pruning basato su RMSE\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\")\n",
    "\n",
    "    model = xgb.train(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=[(dvalid, \"validation\")],\n",
    "        early_stopping_rounds=100,\n",
    "        callbacks=[pruning_callback],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    # Optuna deve minimizzare RMSE\n",
    "    preds = model.predict(dvalid)\n",
    "    rmse = np.sqrt(mean_squared_error(y_valid, preds))\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4e22b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (13561, 14)\n",
      "Target medio: 1.85 (centinaia di k$)\n",
      "\n",
      "--- Training Modello Baseline (Default) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-09 14:50:37,451] A new study created in memory with name: no-name-4ec4167a-abe5-4032-957d-df872e9e9e74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RMSE: 0.3555\n",
      "Numero di pseudo-label confident: 1853 su 2713\n",
      "\n",
      "--- Inizio Tuning con Optuna (GPU + pseudo-label) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-09 14:50:41,436] Trial 0 finished with value: 0.3243772419059875 and parameters: {'max_depth': 13, 'min_child_weight': 20, 'gamma': 0.0029116610566531477, 'subsample': 0.7746123177865296, 'colsample_bytree': 0.8631785585285412, 'lambda': 0.2508904105538494, 'alpha': 1.9230427579001175, 'learning_rate': 0.05109379599479066}. Best is trial 0 with value: 0.3243772419059875.\n",
      "[I 2025-12-09 14:50:44,548] Trial 1 finished with value: 0.3263151648212046 and parameters: {'max_depth': 12, 'min_child_weight': 22, 'gamma': 0.0018826017660091108, 'subsample': 0.8262421180916442, 'colsample_bytree': 0.9021568379167271, 'lambda': 0.39634359150915976, 'alpha': 1.999545456221698, 'learning_rate': 0.058940929242491724}. Best is trial 0 with value: 0.3243772419059875.\n",
      "[I 2025-12-09 14:50:48,577] Trial 2 finished with value: 0.32488464023869995 and parameters: {'max_depth': 11, 'min_child_weight': 22, 'gamma': 0.007187368860336961, 'subsample': 0.8157814391570931, 'colsample_bytree': 0.9039880070375421, 'lambda': 0.8683628645922756, 'alpha': 1.331548856501994, 'learning_rate': 0.04728300761738979}. Best is trial 0 with value: 0.3243772419059875.\n",
      "[I 2025-12-09 14:50:52,518] Trial 3 finished with value: 0.323999341159006 and parameters: {'max_depth': 12, 'min_child_weight': 22, 'gamma': 0.002878625323066173, 'subsample': 0.7518996809524171, 'colsample_bytree': 0.9098704515159247, 'lambda': 0.27715389325673245, 'alpha': 1.8031980246903787, 'learning_rate': 0.04469957751633815}. Best is trial 3 with value: 0.323999341159006.\n",
      "[I 2025-12-09 14:50:55,954] Trial 4 finished with value: 0.32515369005544736 and parameters: {'max_depth': 12, 'min_child_weight': 23, 'gamma': 0.007928690223875514, 'subsample': 0.791219427294774, 'colsample_bytree': 0.8751907889921714, 'lambda': 0.4116236050019876, 'alpha': 1.8295915098244313, 'learning_rate': 0.043150271003294244}. Best is trial 3 with value: 0.323999341159006.\n",
      "[I 2025-12-09 14:50:55,972] Trial 5 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:50:59,582] Trial 6 finished with value: 0.3240796195556942 and parameters: {'max_depth': 11, 'min_child_weight': 15, 'gamma': 0.007662895066961756, 'subsample': 0.8297582387875346, 'colsample_bytree': 0.8640465165852904, 'lambda': 0.4320608775387671, 'alpha': 1.1821303784866914, 'learning_rate': 0.05901649870859951}. Best is trial 3 with value: 0.323999341159006.\n",
      "[I 2025-12-09 14:51:02,973] Trial 7 finished with value: 0.32369371567280775 and parameters: {'max_depth': 12, 'min_child_weight': 22, 'gamma': 0.007329330726716259, 'subsample': 0.8487903601539796, 'colsample_bytree': 0.8521815933229943, 'lambda': 0.20787828456712326, 'alpha': 1.336167029750977, 'learning_rate': 0.06388615939493704}. Best is trial 7 with value: 0.32369371567280775.\n",
      "[I 2025-12-09 14:51:02,989] Trial 8 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:05,134] Trial 9 pruned. Trial was pruned at iteration 219.\n",
      "[I 2025-12-09 14:51:05,972] Trial 10 pruned. Trial was pruned at iteration 109.\n",
      "[I 2025-12-09 14:51:08,716] Trial 11 finished with value: 0.3231847924592894 and parameters: {'max_depth': 12, 'min_child_weight': 20, 'gamma': 0.004359119694098261, 'subsample': 0.7917713055778127, 'colsample_bytree': 0.850500548839292, 'lambda': 0.6010411962875345, 'alpha': 1.6073335711972097, 'learning_rate': 0.062057322551791624}. Best is trial 11 with value: 0.3231847924592894.\n",
      "[I 2025-12-09 14:51:11,228] Trial 12 finished with value: 0.3240464670654889 and parameters: {'max_depth': 13, 'min_child_weight': 19, 'gamma': 0.004874930917248489, 'subsample': 0.7932700374432715, 'colsample_bytree': 0.8538717022177268, 'lambda': 0.6196319622992003, 'alpha': 1.5577420765705359, 'learning_rate': 0.06344812513637893}. Best is trial 11 with value: 0.3231847924592894.\n",
      "[I 2025-12-09 14:51:11,867] Trial 13 pruned. Trial was pruned at iteration 84.\n",
      "[I 2025-12-09 14:51:13,093] Trial 14 pruned. Trial was pruned at iteration 122.\n",
      "[I 2025-12-09 14:51:13,120] Trial 15 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:15,342] Trial 16 pruned. Trial was pruned at iteration 212.\n",
      "[I 2025-12-09 14:51:15,368] Trial 17 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:18,425] Trial 18 finished with value: 0.3236901459798132 and parameters: {'max_depth': 11, 'min_child_weight': 21, 'gamma': 0.003982123867141644, 'subsample': 0.8149711462212745, 'colsample_bytree': 0.868744260093364, 'lambda': 0.755336545930767, 'alpha': 1.7087999857355838, 'learning_rate': 0.06221672951695161}. Best is trial 11 with value: 0.3231847924592894.\n",
      "[I 2025-12-09 14:51:18,706] Trial 19 pruned. Trial was pruned at iteration 38.\n",
      "[I 2025-12-09 14:51:18,736] Trial 20 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:19,644] Trial 21 pruned. Trial was pruned at iteration 109.\n",
      "[I 2025-12-09 14:51:20,140] Trial 22 pruned. Trial was pruned at iteration 58.\n",
      "[I 2025-12-09 14:51:21,222] Trial 23 pruned. Trial was pruned at iteration 120.\n",
      "[I 2025-12-09 14:51:21,668] Trial 24 pruned. Trial was pruned at iteration 53.\n",
      "[I 2025-12-09 14:51:21,696] Trial 25 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:21,723] Trial 26 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:22,410] Trial 27 pruned. Trial was pruned at iteration 78.\n",
      "[I 2025-12-09 14:51:22,437] Trial 28 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:22,468] Trial 29 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:23,188] Trial 30 pruned. Trial was pruned at iteration 83.\n",
      "[I 2025-12-09 14:51:23,215] Trial 31 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:23,757] Trial 32 pruned. Trial was pruned at iteration 60.\n",
      "[I 2025-12-09 14:51:24,098] Trial 33 pruned. Trial was pruned at iteration 39.\n",
      "[I 2025-12-09 14:51:24,127] Trial 34 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:24,158] Trial 35 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:24,186] Trial 36 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:24,220] Trial 37 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:24,249] Trial 38 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:24,977] Trial 39 pruned. Trial was pruned at iteration 78.\n",
      "[I 2025-12-09 14:51:26,318] Trial 40 pruned. Trial was pruned at iteration 147.\n",
      "[I 2025-12-09 14:51:29,413] Trial 41 pruned. Trial was pruned at iteration 308.\n",
      "[I 2025-12-09 14:51:32,685] Trial 42 finished with value: 0.32469284801735887 and parameters: {'max_depth': 13, 'min_child_weight': 22, 'gamma': 0.0036106479957804262, 'subsample': 0.7988050475313887, 'colsample_bytree': 0.8501742426046668, 'lambda': 0.6477325444234094, 'alpha': 1.5848338100556842, 'learning_rate': 0.06295167326984694}. Best is trial 11 with value: 0.3231847924592894.\n",
      "[I 2025-12-09 14:51:32,813] Trial 43 pruned. Trial was pruned at iteration 14.\n",
      "[I 2025-12-09 14:51:33,390] Trial 44 pruned. Trial was pruned at iteration 67.\n",
      "[I 2025-12-09 14:51:33,881] Trial 45 pruned. Trial was pruned at iteration 51.\n",
      "[I 2025-12-09 14:51:34,379] Trial 46 pruned. Trial was pruned at iteration 53.\n",
      "[I 2025-12-09 14:51:34,406] Trial 47 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:34,866] Trial 48 pruned. Trial was pruned at iteration 57.\n",
      "[I 2025-12-09 14:51:36,314] Trial 49 pruned. Trial was pruned at iteration 167.\n",
      "[I 2025-12-09 14:51:36,343] Trial 50 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:37,039] Trial 51 pruned. Trial was pruned at iteration 77.\n",
      "[I 2025-12-09 14:51:37,162] Trial 52 pruned. Trial was pruned at iteration 10.\n",
      "[I 2025-12-09 14:51:37,671] Trial 53 pruned. Trial was pruned at iteration 57.\n",
      "[I 2025-12-09 14:51:38,270] Trial 54 pruned. Trial was pruned at iteration 67.\n",
      "[I 2025-12-09 14:51:38,777] Trial 55 pruned. Trial was pruned at iteration 60.\n",
      "[I 2025-12-09 14:51:38,807] Trial 56 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:38,836] Trial 57 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:39,602] Trial 58 pruned. Trial was pruned at iteration 74.\n",
      "[I 2025-12-09 14:51:39,632] Trial 59 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:39,777] Trial 60 pruned. Trial was pruned at iteration 17.\n",
      "[I 2025-12-09 14:51:39,810] Trial 61 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:39,840] Trial 62 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:39,868] Trial 63 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:39,900] Trial 64 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:40,398] Trial 65 pruned. Trial was pruned at iteration 53.\n",
      "[I 2025-12-09 14:51:40,431] Trial 66 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:40,462] Trial 67 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:40,493] Trial 68 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:40,523] Trial 69 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:51:40,924] Trial 70 pruned. Trial was pruned at iteration 51.\n",
      "[I 2025-12-09 14:51:41,583] Trial 71 pruned. Trial was pruned at iteration 70.\n",
      "[I 2025-12-09 14:51:42,160] Trial 72 pruned. Trial was pruned at iteration 61.\n",
      "[I 2025-12-09 14:51:45,062] Trial 73 finished with value: 0.32355348240209025 and parameters: {'max_depth': 13, 'min_child_weight': 22, 'gamma': 0.003524752892588166, 'subsample': 0.8057645692679599, 'colsample_bytree': 0.8528317349438643, 'lambda': 0.5626703100465819, 'alpha': 1.5482872167244768, 'learning_rate': 0.06534681322226764}. Best is trial 11 with value: 0.3231847924592894.\n",
      "[I 2025-12-09 14:51:45,673] Trial 74 pruned. Trial was pruned at iteration 63.\n",
      "[I 2025-12-09 14:51:46,297] Trial 75 pruned. Trial was pruned at iteration 67.\n",
      "[I 2025-12-09 14:51:48,973] Trial 76 finished with value: 0.32320941173184864 and parameters: {'max_depth': 13, 'min_child_weight': 22, 'gamma': 0.005152464607993996, 'subsample': 0.8019866195879239, 'colsample_bytree': 0.8560736325341011, 'lambda': 0.47519505644011906, 'alpha': 1.4942493294236727, 'learning_rate': 0.06523356918252883}. Best is trial 11 with value: 0.3231847924592894.\n",
      "[I 2025-12-09 14:51:49,750] Trial 77 pruned. Trial was pruned at iteration 84.\n",
      "[I 2025-12-09 14:51:52,609] Trial 78 finished with value: 0.32359579047378334 and parameters: {'max_depth': 12, 'min_child_weight': 22, 'gamma': 0.00584483924014588, 'subsample': 0.8019784887132725, 'colsample_bytree': 0.8531157832653549, 'lambda': 0.41390060611722534, 'alpha': 1.7031081555259449, 'learning_rate': 0.0680411383114029}. Best is trial 11 with value: 0.3231847924592894.\n",
      "[I 2025-12-09 14:51:54,446] Trial 79 pruned. Trial was pruned at iteration 211.\n",
      "[I 2025-12-09 14:51:55,145] Trial 80 pruned. Trial was pruned at iteration 78.\n",
      "[I 2025-12-09 14:51:56,134] Trial 81 pruned. Trial was pruned at iteration 110.\n",
      "[I 2025-12-09 14:51:56,654] Trial 82 pruned. Trial was pruned at iteration 57.\n",
      "[I 2025-12-09 14:51:57,338] Trial 83 pruned. Trial was pruned at iteration 74.\n",
      "[I 2025-12-09 14:51:57,884] Trial 84 pruned. Trial was pruned at iteration 64.\n",
      "[I 2025-12-09 14:51:59,192] Trial 85 pruned. Trial was pruned at iteration 137.\n",
      "[I 2025-12-09 14:51:59,553] Trial 86 pruned. Trial was pruned at iteration 40.\n",
      "[I 2025-12-09 14:52:00,170] Trial 87 pruned. Trial was pruned at iteration 66.\n",
      "[I 2025-12-09 14:52:00,202] Trial 88 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:52:03,815] Trial 89 finished with value: 0.3232062514723402 and parameters: {'max_depth': 13, 'min_child_weight': 21, 'gamma': 0.003500623235998968, 'subsample': 0.7892780633538367, 'colsample_bytree': 0.8566956965935824, 'lambda': 0.5918194751580697, 'alpha': 1.694366130911199, 'learning_rate': 0.06311290183439311}. Best is trial 11 with value: 0.3231847924592894.\n",
      "[I 2025-12-09 14:52:04,597] Trial 90 pruned. Trial was pruned at iteration 81.\n",
      "[I 2025-12-09 14:52:04,626] Trial 91 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:52:05,160] Trial 92 pruned. Trial was pruned at iteration 54.\n",
      "[I 2025-12-09 14:52:05,870] Trial 93 pruned. Trial was pruned at iteration 74.\n",
      "[I 2025-12-09 14:52:05,923] Trial 94 pruned. Trial was pruned at iteration 3.\n",
      "[I 2025-12-09 14:52:05,957] Trial 95 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:52:05,987] Trial 96 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:52:06,017] Trial 97 pruned. Trial was pruned at iteration 0.\n",
      "[I 2025-12-09 14:52:06,556] Trial 98 pruned. Trial was pruned at iteration 59.\n",
      "[I 2025-12-09 14:52:06,607] Trial 99 pruned. Trial was pruned at iteration 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Miglior RMSE trovato da Optuna: 0.3232\n",
      "Migliori parametri: {'max_depth': 12, 'min_child_weight': 20, 'gamma': 0.004359119694098261, 'subsample': 0.7917713055778127, 'colsample_bytree': 0.850500548839292, 'lambda': 0.6010411962875345, 'alpha': 1.6073335711972097, 'learning_rate': 0.062057322551791624}\n",
      "RMSE finale con pseudo-label confident: 0.3349\n",
      "R2 finale con pseudo-label confident: 0.8391\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. Dataset e Split\n",
    "# ==========================================\n",
    "X = data.drop(columns=[\"MedHouseVal\"])\n",
    "y = data[\"MedHouseVal\"]\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_full)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Dataset Shape: {X.shape}\")\n",
    "print(f\"Target medio: {np.mean(y):.2f} (centinaia di k$)\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Baseline (Senza Tuning)\n",
    "# ==========================================\n",
    "print(\"\\n--- Training Modello Baseline (Default) ---\")\n",
    "dtrain_full = xgb.DMatrix(X_train_scaled, label=y_train_full)\n",
    "dtest = xgb.DMatrix(X_test_scaled, label=y_test)\n",
    "\n",
    "params_base = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'tree_method': 'gpu_hist',        \n",
    "    'predictor': 'gpu_predictor',     \n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "model_base = xgb.train(params_base, dtrain_full, num_boost_round=100)\n",
    "preds_base = model_base.predict(dtest)\n",
    "rmse_base = np.sqrt(mean_squared_error(y_test, preds_base))\n",
    "print(f\"Baseline RMSE: {rmse_base:.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2b. Generazione pseudo-label filtrate\n",
    "# ==========================================\n",
    "pseudo_labels = model_base.predict(dtest)\n",
    "\n",
    "# Filtro di confidenza: solo valori vicini alla media del training\n",
    "mean_train = np.mean(y_train_full)\n",
    "std_train = np.std(y_train_full)\n",
    "confident_mask = (pseudo_labels > mean_train - std_train) & (pseudo_labels < mean_train + std_train)\n",
    "\n",
    "X_pseudo_confident = X_test_scaled[confident_mask]\n",
    "y_pseudo_confident = pseudo_labels[confident_mask]\n",
    "\n",
    "print(f\"Numero di pseudo-label confident: {len(y_pseudo_confident)} su {len(y_test)}\")\n",
    "\n",
    "# Combiniamo dati reali + pseudo confident\n",
    "X_combined = np.vstack([X_train_scaled, X_pseudo_confident])\n",
    "y_combined = np.concatenate([y_train_full, y_pseudo_confident])\n",
    "\n",
    "# ==========================================\n",
    "# 3. Funzione Objective con GPU su dati combinati\n",
    "# ==========================================\n",
    "def objective(trial):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "    param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'predictor': 'gpu_predictor',\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 13),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 15, 23),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-3, 0.01),\n",
    "        'subsample': trial.suggest_float('subsample', 0.75, 0.85),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.85, 0.91),\n",
    "        'lambda': trial.suggest_float('lambda', 0.1, 1.0),\n",
    "        'alpha': trial.suggest_float('alpha', 1.0, 2.0),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.04, 0.07),\n",
    "    }\n",
    "\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-rmse\")\n",
    "\n",
    "    model = xgb.train(\n",
    "        param,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        evals=[(dvalid, \"validation\")],\n",
    "        early_stopping_rounds=100,\n",
    "        callbacks=[pruning_callback],\n",
    "        verbose_eval=False\n",
    "    )\n",
    "\n",
    "    preds_val = model.predict(dvalid)\n",
    "    rmse = np.sqrt(mean_squared_error(y_valid, preds_val))\n",
    "    return rmse\n",
    "\n",
    "# ==========================================\n",
    "# 4. Ottimizzazione con Optuna\n",
    "# ==========================================\n",
    "print(\"\\n--- Inizio Tuning con Optuna (GPU + pseudo-label) ---\")\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100, timeout=1200)\n",
    "\n",
    "print(f\"\\nMiglior RMSE trovato da Optuna: {study.best_value:.4f}\")\n",
    "print(\"Migliori parametri:\", study.best_params)\n",
    "\n",
    "# ==========================================\n",
    "# 5. Training Modello Finale\n",
    "# ==========================================\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'predictor': 'gpu_predictor'\n",
    "})\n",
    "best_params['learning_rate'] *= 0.5\n",
    "\n",
    "dtrain_combined = xgb.DMatrix(X_combined, label=y_combined)\n",
    "num_round_final = 5000\n",
    "\n",
    "model_final = xgb.train(\n",
    "    best_params,\n",
    "    dtrain_combined,\n",
    "    num_boost_round=num_round_final,\n",
    "    evals=[(dtest, \"test\")],\n",
    "    early_stopping_rounds=100,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 6. Risultati finali\n",
    "# ==========================================\n",
    "preds_final = model_final.predict(dtest)\n",
    "rmse_final = np.sqrt(mean_squared_error(y_test, preds_final))\n",
    "r2_final = r2_score(y_test, preds_final)\n",
    "\n",
    "print(f\"RMSE finale con pseudo-label confident: {rmse_final:.4f}\")\n",
    "print(f\"R2 finale con pseudo-label confident: {r2_final:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d05c62",
   "metadata": {},
   "source": [
    "**RISULTATO**\n",
    "\n",
    "RMSE Baseline:    0.4718\n",
    "RMSE Ottimizzato: 0.4300\n",
    "\n",
    "param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse', # Metrica per early stopping e pruning\n",
    "        'tree_method': 'hist',\n",
    "        \n",
    "        # --- Struttura ---\n",
    "        # California ha interazioni complesse, permettiamo alberi più profondi\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 15),\n",
    "        # Importante per evitare overfitting su outlier di prezzo\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        \n",
    "        # --- Randomness ---\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        \n",
    "        # --- Regolarizzazione ---\n",
    "        # Fondamentale nella regressione per non inseguire i prezzi estremi\n",
    "        'lambda': trial.suggest_float('lambda', 1e-9, 25.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-9, 25.0, log=True),\n",
    "        \n",
    "        # --- Learning ---\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "    }\n",
    "\n",
    "study.optimize(objective, n_trials=70, timeout=600)\n",
    "\n",
    "=============================================================================== <br>\n",
    "RMSE Baseline:    0.4718\n",
    "RMSE Ottimizzato: 0.4310\n",
    "\n",
    "con scaler\n",
    "\n",
    "param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse', # Metrica per early stopping e pruning\n",
    "        'tree_method': 'hist',\n",
    "        \n",
    "        # --- Struttura ---\n",
    "        # California ha interazioni complesse, permettiamo alberi più profondi\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
    "        # Importante per evitare overfitting su outlier di prezzo\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 25),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-9, 1.5, log=True),\n",
    "        \n",
    "        # --- Randomness ---\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        \n",
    "        # --- Regolarizzazione ---\n",
    "        # Fondamentale nella regressione per non inseguire i prezzi estremi\n",
    "        'lambda': trial.suggest_float('lambda', 1e-6, 20.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-6, 20.0, log=True),\n",
    "        \n",
    "        # --- Learning ---\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.8),\n",
    "    }\n",
    "\n",
    "study.optimize(objective, n_trials=200, timeout=600)\n",
    "\n",
    "=============================================================================== <br>\n",
    "\n",
    "RMSE Baseline:    0.3522\n",
    "RMSE Ottimizzato: 0.3379\n",
    "\n",
    "con scaler\n",
    "\n",
    "study.optimize(objective, n_trials=200, timeout=600)\n",
    "\n",
    "param = {\n",
    "        'verbosity': 0,\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse', # Metrica per early stopping e pruning\n",
    "        'tree_method': 'hist',\n",
    "        \n",
    "        # --- Struttura ---\n",
    "        # California ha interazioni complesse, permettiamo alberi più profondi\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
    "        # Importante per evitare overfitting su outlier di prezzo\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 25),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-7, 1.5, log=True),\n",
    "        \n",
    "        # --- Randomness ---\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        \n",
    "        # --- Regolarizzazione ---\n",
    "        # Fondamentale nella regressione per non inseguire i prezzi estremi\n",
    "        'lambda': trial.suggest_float('lambda', 1e-9, 15.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-9, 15.0, log=True),\n",
    "        \n",
    "        # --- Learning ---\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.6),\n",
    "    }\n",
    "\n",
    "#rimozione outlier\n",
    "numeric_cols = data.select_dtypes(include=[\"float\", \"int\"]).columns.tolist()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = data[col].quantile(0.3)\n",
    "    Q3 = data[col].quantile(0.7)\n",
    "    IQR = Q3 - Q1\n",
    "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "\n",
    "    data[col] = np.where(data[col].between(lower, upper), data[col], np.nan)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "#features combinate\n",
    "data['Ave_Room_Bed'] = data['AveRooms'] / data['AveBedrms']\n",
    "data['HouseRooms'] = data['HouseAge'] / data['AveRooms']\n",
    "\n",
    "Miglior RMSE trovato da Optuna (Validation): 0.3544\n",
    "Migliori parametri: {'max_depth': 13, 'min_child_weight': 23, 'gamma': 3.8838420870962745e-06, 'subsample': 0.7570628531701007, 'colsample_bytree': 0.9109736816364161, 'lambda': 0.02307771384295654, 'alpha': 0.9454327224813569, 'learning_rate': 0.06885420667941514}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
